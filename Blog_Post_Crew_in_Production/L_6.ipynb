{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1834202c-29d0-422f-92ac-f401a5ee173c",
   "metadata": {},
   "source": [
    "# L6: Building Your Crew for Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2db0c-cdb4-4c1d-8d06-9b5523cb04a4",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31fd4d-5688-406e-9824-6a6d5a6abcff",
   "metadata": {},
   "source": [
    "## Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42cdccda-8484-48d1-9c57-36f4365d5400",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "from helper import load_env\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c6290-be77-4e9a-abee-44c3e298d4fc",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d8c71-769e-4c1e-8e6c-46479332cc46",
   "metadata": {},
   "source": [
    "## Creating a new project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9cdff5-a397-4b18-8f34-93473a8741cc",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mCreating folder new_project...\u001b[0m\r\n",
      "\u001b[33m\tFolder new_project already exists.\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/.gitignore\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/pyproject.toml\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/README.md\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/__init__.py\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/main.py\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/crew.py\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/tools/custom_tool.py\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/tools/__init__.py\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/config/agents.yaml\u001b[0m\r\n",
      "\u001b[32m  - Created new_project/src/new_project/config/tasks.yaml\u001b[0m\r\n",
      "\u001b[32m\u001b[1mCrew new_project created successfully!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! crewai create crew new_project --provider openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7a0f2-7302-42af-8963-3dbc2ef781dc",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1cf0cf-d40b-4aa1-bc28-6fdce3952c27",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note</code>:</b> The following line might take a few minutes to finish.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec8d5c6-5049-4f3f-b94d-682a0ae5bd03",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m222 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m new-project\u001b[2m @ file:///home/jovyan/work/L13/new_project\u001b[0m        \n",
      "\u001b[2K\u001b[1A   \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m new-project\u001b[2m @ file:///home/jovyan/work/L13/new_project\u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 193ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.28ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m(from file:///home/jovyan/work/L13\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mnew-project\u001b[0m\u001b[2m==0.1.0 (from file:///home/jovyan/work/L13/new_project)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! cd new_project && crewai install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88c796-8cea-4c46-90b2-0c884423dde7",
   "metadata": {},
   "source": [
    "## Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8e0ac1-0246-43bc-a99a-7a3fbeb2a1c0",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY=YOUR_API_KEY_HERE\r\n"
     ]
    }
   ],
   "source": [
    "! cat new_project/.env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b3b03-6ce9-4351-a738-aff246437111",
   "metadata": {},
   "source": [
    "## Running the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe150f6-f8c5-42a9-82df-391eabc9602d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the Crew\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2024.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. **Advancements in Fine-Tuning Techniques**: Researchers are developing more efficient methods for fine-tuning large language models (LLMs), resulting in models that can be customized for specific tasks with fewer data and computing resources. Techniques such as LoRA (Low-Rank Adaptation) and QLoRA have emerged to make fine-tuning more accessible and effective.  \n",
      "\n",
      "2. **Incorporation of Multimodal Capabilities**: AI LLMs are now being enhanced with multimodal capabilities, allowing them to process and generate not just text, but also images, audio, and video. This has significantly improved their application in fields like education, entertainment, and research by enabling more interactive and enriched user experiences.  \n",
      "\n",
      "3. **Development of More Efficient Architectures**: New architectural innovations such as sparse transformers and mixture of experts (MoE) are being implemented to enhance the performance and efficiency of LLMs. These advancements aim to reduce the computational resources needed while maintaining or improving language understanding and generation quality.  \n",
      "\n",
      "4. **Increased Focus on Ethical AI**: The field has seen a growing commitment to addressing ethical concerns surrounding AI LLMs, including biases in language models and their impact on society. Researchers and organizations are actively pursuing methods for bias detection, mitigation, and the implementation of ethical guidelines in AI systems.  \n",
      "\n",
      "5. **Improvements in Contextual Understanding**: AI LLMs have made remarkable progress in understanding context, leading to improved performance in conversation and content generation. Enhanced abilities to track multi-turn dialogues and infer user intent are helping create more human-like interactions.  \n",
      "\n",
      "6. **Resource-Efficient Inference Methods**: With a push towards sustainability, new approaches are being developed that reduce the carbon footprint of AI LLMs during inference. Techniques such as quantization and efficient model pruning contribute to lower energy consumption while preserving accuracy.  \n",
      "\n",
      "7. **Emerging Platforms for Civilian Use**: Several platforms are emerging that provide access to advanced LLMs for civilian use, enabling businesses and individuals to leverage AI capabilities for content creation, customer service, and data analysis without needing extensive technical knowledge.  \n",
      "\n",
      "8. **Shift Towards Open-Source Models**: While proprietary LLMs dominate the landscape, there is a growing movement towards the development and deployment of open-source models. These models promote transparency, collaboration, and accessibility, allowing more researchers and developers to contribute and innovate.  \n",
      "\n",
      "9. **Integrating LLMs in Decentralized Applications**: AI LLMs are increasingly being integrated into decentralized applications (dApps), where users maintain control of their data and transactions. This trend is fostering the development of privacy-centric applications that leverage AI without compromising user security.  \n",
      "\n",
      "10. **Regulatory Development and Compliance**: As AI LLMs become more ubiquitous, governments and organizations are working on regulatory frameworks to ensure safe and responsible AI deployment. Regulatory measures are aiming to create standards for transparency, accountability, and users' rights in relation to AI technologies.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mReview the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Detailed Report on Advancements in AI LLMs\n",
      "\n",
      "## 1. Advancements in Fine-Tuning Techniques\n",
      "Recent innovations in fine-tuning techniques for large language models (LLMs) have transformed how these models are adapted for specific tasks. Researchers have developed methods like Low-Rank Adaptation (LoRA) and QLoRA, which allow for the fine-tuning process to be more efficient, requiring less data and computational resources compared to traditional approaches. LoRA works by introducing low-rank matrices into the neural architecture, enabling the model to adapt its performance to new tasks with minimal intervention. QLoRA leverages quantization techniques to improve the efficiency of these adaptations further. These advancements not only enhance accessibility for smaller organizations and individual developers but also democratize the use of LLMs across various industries, such as healthcare and finance, where task-specific applications are paramount.\n",
      "\n",
      "## 2. Incorporation of Multimodal Capabilities\n",
      "AI LLMs are increasingly incorporating multimodal capabilities, extending their functionality beyond text to include images, audio, and even video content. This evolution allows for enriched interactions and a more comprehensive understanding of human communication. By integrating multiple modes of information processing, LLMs are significantly improving applications in diverse sectors. In education, learners can engage with dynamic content that includes animated illustrations alongside textual explanations. In entertainment, AI-generated videos that respond to natural language prompts are becoming more common. This multimodal approach enhances user experience, making interactions more engaging and efficient across various platforms.\n",
      "\n",
      "## 3. Development of More Efficient Architectures\n",
      "Emerging architectural innovations, such as sparse transformers and mixture of experts (MoE), are designed to optimize the performance and computational efficiency of LLMs. Sparse transformers facilitate better data handling by only activating portions of the model relevant to specific tasks, significantly reducing the number of computations required. Similarly, MoE architectures utilize select subsets of models to process inputs, allowing for increased performance without the need for proportional increases in resources. These developments aim to enhance language comprehension and generation while addressing the growing concerns regarding the environmental impact of extensive computational demands.\n",
      "\n",
      "## 4. Increased Focus on Ethical AI\n",
      "The rise of AI LLMs has prompted serious discussions around ethical practices in AI development. Researchers are increasingly committed to identifying and mitigating biases present in language models, recognizing their potential societal impacts. Initiatives are underway to establish comprehensive frameworks aimed at bias detection and rectification, promoting equitable AI systems. Ethical guidelines are being developed to ensure transparency and accountability in AI usage. Organizations are forming partnerships to create best practices and share findings on ethical dilemmas, thus fostering a responsible AI ecosystem that prioritizes fairness and user trust.\n",
      "\n",
      "## 5. Improvements in Contextual Understanding\n",
      "Improvements in contextual understanding have led to notable advancements in how AI LLMs process conversational data and generate content. With enhanced abilities to grasp multi-turn dialogues and discern user intent, these models now provide more human-like interactions. Significant progress in utilizing contextual cues enables LLMs to maintain coherence over longer conversations and tailor responses effectively to user inquiries. This heightened understanding not only elevates customer service applications but also enriches creative content generation, where an intuitive grasp of context is essential for producing meaningful and relevant outputs.\n",
      "\n",
      "## 6. Resource-Efficient Inference Methods\n",
      "As the demand for more sustainable AI technologies grows, various resource-efficient inference methods are being developed. Techniques such as model quantization and pruning help to reduce the carbon footprint associated with AI LLM operations. Model quantization compresses the representations used in computations, leading to lower energy usage during processing without a significant loss in accuracy. Pruning involves removing unnecessary weights from networks, making models lighter and faster while maintaining performance. These innovations are crucial in promoting environmental sustainability within the AI sector, demonstrating that efficiency does not need to compromise effectiveness.\n",
      "\n",
      "## 7. Emerging Platforms for Civilian Use\n",
      "Numerous platforms are emerging, aimed at providing civilian access to sophisticated LLMs, thereby enabling businesses and individuals to utilize AI tools for various applications. These platforms often feature user-friendly interfaces that cater to non-technical users, allowing them to harness the power of AI for tasks such as content generation, data analysis, and customer service. As a result, the entry barrier to utilizing advanced LLM capabilities is significantly lowered. This accessibility equips a broader audience with the means to enhance productivity and creativity, bridging the gap between sophisticated AI technologies and everyday users.\n",
      "\n",
      "## 8. Shift Towards Open-Source Models\n",
      "While proprietary LLMs have traditionally dominated the AI landscape, a noticeable shift toward developing and deploying open-source models is taking place. This movement is driven by the desire for transparency, collaboration, and innovation within the research community. Open-source models allow developers and researchers to access, modify, and improve upon existing frameworks, resulting in faster advancements and a more diverse set of applications. By fostering an inclusive environment, the open-source movement empowers contributors to address unique challenges and encourages the diffusion of knowledge and ideas across global research networks.\n",
      "\n",
      "## 9. Integrating LLMs in Decentralized Applications\n",
      "The integration of AI LLMs into decentralized applications (dApps) is gaining traction, emphasizing user control over data and transactions. This trend supports the development of privacy-centric applications while leveraging AI capabilities. By enabling users to engage with LLMs directly in a decentralized framework, developers can design solutions that prioritize security and data sovereignty. This approach not only enhances user trust but fosters innovation in areas such as finance, social media, and online collaboration, where decentralized technologies can offer new functionalities and capabilities.\n",
      "\n",
      "## 10. Regulatory Development and Compliance\n",
      "As AI LLMs become increasingly prevalent, regulatory development and compliance are crucial to ensuring the safe and responsible deployment of these technologies. Governments and organizations are actively working to establish regulatory frameworks that create standards for transparency, accountability, and user rights concerning AI technologies. These frameworks aim to mitigate risks associated with the misuse of AI and provide clear guidelines for ethical usage. Ongoing dialogue between stakeholders, including policymakers, researchers, and industry leaders, is essential to navigate the complexities of AI regulation effectively, ensuring that the benefits of LLMs are harnessed responsibly while addressing public concerns.\n",
      "\n",
      "This report outlines critical advancements and trends in AI large language models, providing insights into the current landscape and future directions within this rapidly evolving field.\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! cd new_project && crewai run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdaac0e-928b-49d4-9c46-714358a5d178",
   "metadata": {},
   "source": [
    "## Flows CLI - Command Line Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd6171b-8432-4aaa-bfb3-01a5153b4ead",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mCreating flow new_flow...\u001b[0m\n",
      "\u001b[32m\u001b[1mFlow new_flow created successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! crewai create flow new_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3c74aa-35cf-43f1-8ea8-0cafa2dda991",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\r\n",
      "pyproject.toml\r\n",
      "src\r\n",
      "tests\r\n"
     ]
    }
   ],
   "source": [
    "! ls -1 new_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162027cd-f13e-4d97-9274-71f633f28c4f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\r\n",
      "crews\r\n",
      "main.py\r\n",
      "tools\r\n"
     ]
    }
   ],
   "source": [
    "! ls -1 new_flow/src/new_flow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6b0c7-9844-4fd0-832b-94e3410e512b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8b501-7000-4091-8814-eed692ddbcf5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24501ada-f1fb-48ec-a637-e2659ed7cf16",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a5ea6-e85b-4ebe-9120-17f15a2db1b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7267b-8d41-4033-a69a-f72cb10308af",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc56040-f033-4b18-913f-dd68a4cd3fff",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
